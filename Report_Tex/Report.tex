%% LyX 2.2.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[a4paper,english]{article}
\usepackage[LGR,T1]{fontenc}
\usepackage[utf8]{inputenc}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\special{papersize=\the\paperwidth,\the\paperheight}

\DeclareRobustCommand{\greektext}{%
  \fontencoding{LGR}\selectfont\def\encodingdefault{LGR}}
\DeclareRobustCommand{\textgreek}[1]{\leavevmode{\greektext #1}}
\ProvideTextCommand{\~}{LGR}[1]{\char126#1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

\usepackage{textgreek}
\usepackage[english]{babel}
\usepackage[style=numeric, backend=biber]{biblatex}
\usepackage{csquotes}
\addbibresource{bibliography.bib}
%opening
\title{RoboCup Final Project Report \\ Color Detection }
\author{Jakob Stimpfl \and Charlotte Burmeister}

\makeatother

\usepackage{babel}
\begin{document}
\maketitle \tableofcontents{}

\section{Introduction}

The Nao robot is a humanoid robot produced by Aldebaran. It is 574
mm high and has 26 joints, controlled by stepper motors.

In this project to goal was to enable the robot to listen to a color,
interpret the color heard as RGB values, search for that color in
a set distance in front of him and then point to this color.

\section{Implementation}

As predetermined by the course the Python Software Development Kit
(SDK) was used. This SDK provides the use of C++ modules and enables
the user to create own Python modules.\cite{API}

\subsection{Speech Detection}

The first part of the project aims at implementing a speech recognition
and distinguish the different colors. Therefore the module ``ALSpeechRecognition''
was used. It provides the event listener ``WordRecognized''. When
subscribed to this module, an event is thrown when a word is recognized
from the vocabulary list which is specified beforehand. Once the event
is detected and thrown, the program jumps to the method ``OnColorHeard'',
there a simple if-query determines the color. Afterwards the next
method is called with tho colors Red-Green-Blue values which is described
in the following section. 

\subsection{Color Detection}

Since the API provides a module ``ALColorBlobDetection'' for finding
a color-blob we could use that one. To find the color we want to search
for, we have to set its RGB-Values and a threshold with the methode
``setColor(r, g, b, th)''. By try and error we found out, that a
threshold of 50 seems to produce good results. Additionaly we set
the minimal size of our blob to 10 pixels and its distance depending
the distance we want to search for in: ``setObjectProperties(10,
distance)''. Then we subscribe for the ``onColorDetected''-event.
If that event is thrown in the eventhandler we use the ``getCircle()''-methode
to determine the to the width of our picture relative x-value. 

\subsection{Movement}

As described in the previuos section at this point we have an x-value
which we can use to calculate the angle we have to moove our arm in,
to point at our object. As a first distinction we determine whether
this x-value is greater than 0.5. In that case we use the right arm
otherwise the left. Since the calculation for the left arm is the
same then for the right with the small difference, that the pointing-angle
has to be negated, we will subsequently explain the calculations only
for the right arm. 

Since we have the normalized x-value counted from the left side of
the picture we have to calculate the horizontal distance from the
robots center by multiplying it with the horizon-lenght which in by
our definition is the size an object could maximal have, in the set
distance, so that the robot could still see all of it. For that we
need annother parameter, by name, the opening angle Î± of our camera,
defined in the documentation \cite{DOC}. With the law of sine and
the distance d of our robot we can determine now the half horizen
length by $\frac{horizon}{2}=d*\frac{sin(\frac{\alpha}{2})}{sin(90-\frac{\alpha}{2})}$

Given the distance robot to object and the horizontal distance from
the robots center (x-value) we 

\section{Difficulties and future prospects}

\subsection{Expressive Listening}

\subsection{Blob detection}

As a future improvement the minimal blob size should be adjusted dependending
on the distance if the size of the searched object is given.

\section{Conclusion}

\paragraph{Goals reached}

The robot is able to listen to a color, detect this color and point
towards it. 

\paragraph{Limitations}

So far only the colors which are specified in the code can be detected
by the robot. This limits the interaction possiblities. A more interactive
approach would be that any color could be said or that the words can
be specified by the person interacting with the robot, e.g. via speech
input. But permitting the robot to detect more colors is difficult,
as colors appear quite different in changing light conditions. Distinguishing
black, red, blue and green is easier as the RGB values are very different.

\printbibliography
\end{document}
